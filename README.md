# ai-data-pipeline-sagemaker

## Developer: Andrew Kremmidas â€” AWS Solutions Architect Associate

AI Data Pipeline â€” SageMaker Model Training & Inference
End-to-End Machine Learning Workflow on AWS



A fully managed AI data pipeline that handles:
#1 Data ingestion
#2 Data preprocessing + feature engineering
#3 Model training and tuning (SageMaker)
#4 Model deployment with API inference
#5 MLOps automation + monitoring
This project demonstrates modern cloud-native machine learning engineering â€” great for enterprise ML workloads.

Real-World Architecture
Raw Data Source (CSV/JSON/API)
           â”‚
           â–¼
  S3 Data Lake (raw zone)
           â”‚
   Trigger / Schedule
           â–¼
    Lambda or SageMaker Processing
       (clean + feature engineering)
           â”‚
           â–¼
 S3 Data Lake (processed zone)
           â”‚
           â–¼
    SageMaker Training Job
           â”‚
           â–¼
    Model Registry + Artifacts
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â–º Batch Inference (SageMaker)
           â”‚
           â–¼
   SageMaker Endpoint (Real-Time API)
           â”‚
           â–¼
     API Gateway + Cognito Auth

Features & Capabilities
Capability	Demonstrates
Automated ETL in cloud	Data engineering fundamentals
Managed model lifecycle	Production-ready ML
Infra-as-Code (future)	DevOps + reproducibility
Scalable endpoints	Global AI applications
Cloud-native observability	Logs, metrics, cost control
The goal: automated AI delivery pipeline â€” no training on laptops.

AWS Services Used
Amazon S3 â€” Data ingestion & lake storage
AWS Lambda â€” Trigger data preprocessing
Amazon SageMaker
Processing Jobs: Data cleaning
Training Jobs: Managed compute
Endpoints: Real-time inference serving
Model Registry: Versioning + rollbacks
Amazon CloudWatch â€” Monitoring model drift + latency
Amazon API Gateway â€” Public API for inference
Cognito (optional) â€” Authentication & access control



Training Script Example
(Python â€” packaged for SageMaker)
import argparse
import os
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import joblib

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data-dir", type=str)
    parser.add_argument("--model-dir", type=str)
    args = parser.parse_args()

    train = pd.read_csv(os.path.join(args.data_dir, "train.csv"))
    X = train.drop("label", axis=1).values
    y = train["label"].values

    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=20
    )
    model.fit(X, y)

    joblib.dump(model, os.path.join(args.model_dir, "model.joblib"))
More scripts coming soon in /src directory.

Deployment Steps (High-Level)
Step	Action
1	Push raw dataset to S3
2	Run Processing Job to transform data
3	Execute Training Job â†’ produce model artifact
4	Register model in SageMaker Model Registry
5	Deploy Model Endpoint
6	Query endpoint via API Gateway
I can generate CLI + CDK scripts for one-click automation.
ğŸ” Example Inference Request
curl -X POST \
  -H "Content-Type: application/json" \
  -d '{"data":[5.1, 3.5, 1.4, 0.2]}' \
  "$INVOKE_URL/predict"
Response:
{ "prediction": "Iris-setosa" }

Security Best Practices
âœ” Private subnets and VPC endpoints
âœ” IAM role separation of duties
âœ” CloudWatch anomaly monitoring
âœ” S3 + EBS encryption enabled
âœ” Endpoint throttling + API auth

Repo Structure
project-3/
â”œâ”€ src/
â”‚  â”œâ”€ processing/
â”‚  â”œâ”€ training/
â”‚  â”œâ”€ inference/
â”œâ”€ diagrams/
â”‚  â””â”€ sage-ai-pipeline.png
â”œâ”€ pipeline-config/
â””â”€ README.md

Future Enhancements
Hyperparameter Tuning (SageMaker HPO)
Canary or Blue/Green endpoint deployments
Model drift detection with SageMaker Monitor
EMR or Glue data engineering integration
CI/CD with CodePipeline (MLOps)

## Overall Project Goal
Create a production-grade AI pipeline that continuously improves using scalable, automated cloud services â€” the foundation of real ML businesses.
